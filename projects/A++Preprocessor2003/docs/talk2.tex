\documentclass[10pt]{llncs}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}

% Fixme margin note package (turn on draft mode)
% \usepackage[final]{fixme}
% \usepackage[draft]{fixme}

\usepackage{color,listings}
\lstloadlanguages{APP}


\psfigdriver{dvips}

% New commands
\newcommand{\commentout}[1]{}

% change the title of the Fixme List
% \renewcommand{\listfixmename}{Things to Fix}

\newcommand{\comm}[2]{\bigskip
                      \begin{tabular}{|p{11cm}|}\hline
                      \multicolumn{1}{|c|}{{\bf Comment by #1}}\\ \hline
                      #2\\ \hline
                      \end{tabular}
                      \bigskip
                     }

\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\evensidemargin}{-1.0in}
\addtolength{\textwidth}{2.0in}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.5in}

% \pagenumbering{roman}
% \pagestyle{empty}
% \setcounter{page}{0}
% \thispagestyle{empty}

\sloppy

%---------------------------------------------------------------------
% Begin Document
%---------------------------------------------------------------------

\commentout{ Outline for Paper }

\begin{document}

\title{ Introduction to P++ (DRAFT DOCUMENT: Day 2) }
\author{Daniel J.\ Quinlan }
\institute{Center for Applied Scientific Computing\\
Lawrence Livermore National Laboratory, Livermore, CA, USA\\
}

\date{}

\maketitle

\begin{abstract}
   Talk on P++.
\end{abstract}

\lstset{
 % language=C++,
   language=APP,
 % basicstyle=\small,                 % print whole listing small
   basicstyle=\scriptsize,            % print whole listing scriptsize
 % keywordstyle=\color{red}\bfseries\underbar, % underlined bold red keywords
 % keywordstyle=\bfseries\underbar, % underlined bold red keywords
   keywordstyle=\color{red}\bfseries, % underlined bold red keywords
   ndkeywordstyle=\color{blue}, % underlined bold red keywords
 % rdkeywordstyle=\color{blue}, % underlined bold red keywords
   identifierstyle={},                % nothing happens to other identifiers
 % identifierstyle=\color{blue},                % nothing happens to other identifiers
   commentstyle=\color{green},                % nothing happens to other identifiers
 % stringstyle=\ttfamily,             % typerwriter type for strings
 % stringstyle=\color{cyan}\ttfamily,             % typerwriter type for strings Magenta
   stringstyle=\color{magenta}\ttfamily,             % typerwriter type for strings Magenta
   stringspaces=false,                % no special string spaces
   labelstyle=\scriptsize,
   labelstep=1,
   labelsep=10pt
}

\section{Introduction}

    Within P++, the paritioning of data is the single largest additional 
feature added to A++ since all array oparations work identically in both P++
as A++. the partitioning of data can be controled in many ways.

The following test code from P++ demonstrates the use of a few of the
features of the partitioning mechanism to redistribute data across
different numbers of processors.

\vspace{0.5in}

\begin{lstlisting}{}

  //============================================================
  // ... test modification of partitioning object ...

     if (Number_Of_Processors > 2)
        {
          int Size = 10;
          Partitioning_Type Partition (Range(0,0));
          TYPEArray AAA(Size,Partition);
          TYPEArray BBB(Size*2,Partition);
          TYPEArray CCC(Size/2,Partition);
          TYPEArray DDD(Size*2,Partition);
          TYPEArray EEE(Size/2,Partition);
          TYPEArray FFF(Size,Partition);

          int i;
          for (i=0; i< Number_Of_Processors; i++)
             {
               Partition.SpecifyProcessorRange (Range(0,i));
             }
          for (i=0; i< Number_Of_Processors; i++)
             {
               Partition.SpecifyProcessorRange (Range(i,Number_Of_Processors-1));
             }

       // Allocate more data to test partitioning (after redistribution)
          TYPEArray GGG(Size,Partition);
          TYPEArray HHH(Size*2,Partition);
          TYPEArray III(Size/2,Partition);
          TYPEArray JJJ(Size,Partition);

          cout << "Partitioning Object modification test okay" << endl;
        }

  // ============================================================

\end{lstlisting}

\newpage
\section{Parallel Printf}
    Debugging parallel applications can be problematic.  Often the I\/O generated by printf is mixed
with the output from oather processors is ways that make it difficult to debug parallel application.
The following output demonstrates a utility witin P++ to simplify the I\/O for applications.  There
are two function parallelPrintf(...) and fancyParallelPrintf(...).

parallelPrintf(...) maintains the exact same semantics as printf a \#define maps it to printf
currently.  fancyParallelPrintf(...) alters the semantics of printf to enforce a global
communication.  The user may optionally map it to printf (indirectly by mapping it to parallelPrintf
(\#define parallelPrintf fancyParallelPrintf).

\vspace{0.5in}

\begin{lstlisting}{}
NODE -1: A++ Internal_Index bounds checking: ON 
NODE -1: No application program name specified (searching internally for correct name ...) 
NODE -1: Application_Program_Name = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE -1: Inside Communication_Manager::Start_Parallel_Machine (MPI version) (APP_DEBUG = 0) 
NODE  0: 
NODE  0: ***************************************************** 
NODE  0: P++ Virtual Machine Initialized: 
NODE  0:      Process Number                 = 0 
NODE  0:      UNIX process ID                = 21346 
NODE  0:      Process located on machine     = koch.llnl.gov 
NODE  0:      Number_Of_Processors           = 4 
NODE  0:      Local directory (for file I/O) = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests  
NODE  0:      Application_Program_Name       = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE  0:      Default communication model    = VSG everywhere 
NODE  0: ***************************************************** 
NODE  0: 
NODE -1: A++ Internal_Index bounds checking: ON 
NODE -1: No application program name specified (searching internally for correct name ...) 
NODE -1: Application_Program_Name = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE -1: Inside Communication_Manager::Start_Parallel_Machine (MPI version) (APP_DEBUG = 0) 
NODE  2: 
NODE  2: ***************************************************** 
NODE  2: P++ Virtual Machine Initialized: 
NODE  2:      Process Number                 = 2 
NODE  2:      UNIX process ID                = 21370 
NODE  2:      Process located on machine     = koch.llnl.gov 
NODE  2:      Number_Of_Processors           = 4 
NODE  2:      Local directory (for file I/O) = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests  
NODE  2:      Application_Program_Name       = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE  2:      Default communication model    = VSG everywhere 
NODE  2: ***************************************************** 
NODE  2: 
NODE -1: A++ Internal_Index bounds checking: ON 
NODE -1: No application program name specified (searching internally for correct name ...) 
NODE -1: Application_Program_Name = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE -1: Inside Communication_Manager::Start_Parallel_Machine (MPI version) (APP_DEBUG = 0) 
NODE  1: 
NODE  1: ***************************************************** 
NODE  1: P++ Virtual Machine Initialized: 
NODE  1:      Process Number                 = 1 
NODE  1:      UNIX process ID                = 21358 
NODE  1:      Process located on machine     = koch.llnl.gov 
NODE  1:      Number_Of_Processors           = 4 
NODE  1:      Local directory (for file I/O) = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests  
NODE  1:      Application_Program_Name       = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE  1:      Default communication model    = VSG everywhere 
NODE  1: ***************************************************** 
NODE  1: 
NODE -1: A++ Internal_Index bounds checking: ON 
NODE -1: No application program name specified (searching internally for correct name ...) 
NODE -1: Application_Program_Name = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE -1: Inside Communication_Manager::Start_Parallel_Machine (MPI version) (APP_DEBUG = 0) 
NODE  3: 
NODE  3: ***************************************************** 
NODE  3: P++ Virtual Machine Initialized: 
NODE  3:      Process Number                 = 3 
NODE  3:      UNIX process ID                = 21382 
NODE  3:      Process located on machine     = koch.llnl.gov 
NODE  3:      Number_Of_Processors           = 4 
NODE  3:      Local directory (for file I/O) = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests  
NODE  3:      Application_Program_Name       = /home/dquinlan2/A++P++/SUN_CC_NOPADRE/DEBUG_NOPURIFY/P++/dqDeveloperTests/test2002_01 
NODE  3:      Default communication model    = VSG everywhere 
NODE  3: ***************************************************** 
NODE  3: 
NODE  0: Test of printf function 1 
NODE  0: Test of printf function 2 
NODE  0: Test of printf function 3 
NODE  0: Test of printf function 4 
NODE  0: Test of printf function 5 
Inside of Stdio_Print_From_All_Nodes! 
NODE  2: Test of printf function 1 
NODE  2: Test of printf function 2 
NODE  2: Test of printf function 3 
NODE  2: Test of printf function 4 
NODE  2: Test of printf function 5 
Inside of Stdio_Print_From_All_Nodes! 
NODE  3: Test of printf function 1 
NODE  3: Test of printf function 2 
NODE  3: Test of printf function 3 
NODE  3: Test of printf function 4 
NODE  3: Test of printf function 5 
Inside of Stdio_Print_From_All_Nodes! 
NODE  1: Test of printf function 1 
NODE  1: Test of printf function 2 
NODE  1: Test of printf function 3 
NODE  1: Test of printf function 4 
NODE  1: Test of printf function 5 
Inside of Stdio_Print_From_All_Nodes! 
NODE 0 (and nodes  1 2 3): Test of parallelPrintf function 1 
NODE 0 (and nodes  1 2 3): Test of parallelPrintf function 2 
NODE 0 (and nodes  1 2 3): Test of parallelPrintf function 3 
NODE  2: Program Terminated Normally! 
NODE  2: Exiting P++ Virtual Machine! 
NODE  1: Program Terminated Normally! 
NODE  1: Exiting P++ Virtual Machine! 
NODE  0: Program Terminated Normally! 
NODE  0: Exiting P++ Virtual Machine! 
NODE  3: Program Terminated Normally! 
NODE  3: Exiting P++ Virtual Machine! 

\end{lstlisting}



\newpage
\section{Development of P++}
   The following listing is from the file A++P++/README.DEVELOPER.  It details
how to setup A++/P++ for development work.

\vspace{0.5in}

\begin{lstlisting}{}
*** A++P++ Developer Readme file

Some brief directions on how to use the development version of A++P++

HOW TO USE A++P++:
  1) Checkout a NEW version from CVS:
     In your .cshrc set the variable CVSROOT to /usr/casc/overture/A++P++/A++P++CVS/A++P++_Repository
     (setenv CVSROOT /usr/casc/overture/A++P++/A++P++CVS/A++P++_Repository)
     Then run "source ~/.cshrc" to have this environment variable set properly.
     Then run cvs to checkout the current version from A++P++.
     cvs co A++P++

  2) Update and EXISTING version from CVS:
     Run "cvs update" from inside the A++P++ directory (at the top level) to update
     and existing version of A++P++ with the new changes within the CVS repository.

  3) After being checked out (or updated) from CVS:
     Run the "build" script in the top level A++P++ directory to build all configure scripts
     and Makefile.in files (using automake). To generate all the source code for A++P++, you 
     must also run "make -f makefile.build all_source". This is half of the difference between
     the deveopment environment and the distribution, the other half is the additional
     dependencies of the source code upon the macro files that are used to generate both A++
     and P++. Note that the makefile.build is setup to use "gm4" instead of "m4" so this
     can be a problem on some systems. Only the makefile.build in the A++/src/MDI and P++/src/MDI
     directories need to be edited to change this (or you can use a link to ln -s /usr/bin/m4 ~/bin/gm4
     if you have ~/bin in your path).  The makefile.build is used to build the MDI source only,
     the regular Makefile has the dependencies requred to build all other source code.

  4) Build a compile directory (for the compile tree):
     Make a separate directory to be the root of the compile tree. There can be many compile 
     trees if you want. Alternatively you CAN compile in the source tree but this then limits
     your ability to build multiple version of A++P++ with different options (debugging,
     optimization, etc.)

  ** Before the next step be sure you are using the correct compiler (check your path) and 
     that you are using the correct version of automake (on the SUN this is located in the
     default directories (version 1.5 of Automake and version 2.52 of Autoconf)).  The directory
     /usr/casc/overture/local-sparc64-sun-solaris2.7/bin should also appear in your path
     since this directory contains current versions of GNU tools (e.g. tar) that are used 
     internally.

**5) Running configure from within a compile tree:
     Run

          <pathToSourceTree>/configure

     Type "<pathToSourceTree>/configure --help=recursive" to see the different configuration options.
     Note that the configure scripts in A++ and P++ contain options that are specific
     to A++ and P++.  Also it is often helpful to remove the config.cache file before rerunning
     configure.

     "<pathToSourceTree>" is meant to be the absolute or relative path to the source tree
     where the CVS version was checked out.  After options have been selected, type
     "<pathToSourceTree>/configure <selected-options>" to run the configure script.
     Running the configure script with no options is sufficient.

**6) Running Make after running configure:
     After configuration (after the configure script is finished) run "make" or "gmake".
     If you have a development version then you can also make distributions by running
     "make dist".  If you want to build a new distribution AND test it, you have to
     run "make distcheck" (either make or gmake will work, I think).

  7) Testing your new version of A++P++:
     Automated tests are available within the distribution of A++P++, to run these tests
     type "make check".  Tests on a SUN Sparc 10 currently take about 10 minutes to run
     unless you configure with DQ's development directory, in which case the tests take 
     about an hour.

DIFFERENCE BETWEEN DEVELOPER AND DISTRIBUTION VERSIONS:

The A++P++ source code works slightly differently from most average
autoconf-controlled source code.  In most autoconf-controlled
sources, one one uses autoconf (and possibly automake) to
set up the configure process, followed by the "./configure"
command, followed by the "make" command.

In the developer's version of A++P++, many source codes are
similar enough that they are not explicitly kept where the compiler
would look for them.  Rather, they are regenerated using links
and m4 files (in the Common_Code directory). So before autoconf is
even used, we actually have to build the missing source codes.
This generates codes for various types (floats, doubles, int, etc).
The command for this is (as of this writing)

  make -f makefile.build all_source

Naturally, makefile.build is NOT an autoconf-controlled makefile--it
has to be there before autoconf is run.

The A++P++/makelinks script is also there to build the necessary
links before the code is compiled.

In the distribution version of A++P++, the
source files and links have all been pregenerated.  So the
"make -f makefile.build all_source" and "makelinks" commands
are not neccessary.  The user only has to download the distribution
tar file, configure, install and use.  It is NOT true that we
distribute the m4 sources, although that may conceivably be done.

\end{lstlisting}


\newpage
\section{Scalar Indexing in P++}
   Scalar indexing in P++ is quite expensive, but it works and it is robust.

\subsection{The C++ Scalar Indexing Operator}
   This is essentailly a trivial function for A++, but a bit more complex for P++.

\vspace{0.5in}

\begin{lstlisting}{}
inline int & 
intSerialArray::operator() ( int i , int j ) const
{
#if COMPILE_DEBUG_STATEMENTS
   if (APP_DEBUG > 0)
      printf ("Inside of intSerialArray::operator() ( int i=%d , int j=%d ) \n",i,j);
   Test_Consistency();
#endif

   APP_ASSERT (usesIndirectAddressing() == FALSE);

#if defined(BOUNDS_CHECK)
   Integer_Pointer_Array_MAX_ARRAY_DIMENSION_Type Scalar_Index_List;
   Scalar_Index_List [0] = &i;
   Scalar_Index_List [1] = &j;
   for (int temp_index=2; temp_index < MAX_ARRAY_DIMENSION; temp_index++)
      Scalar_Index_List[temp_index] = NULL;
   Array_Descriptor.Error_Checking_For_Scalar_Index_Operators ( Scalar_Index_List );
#endif

   int Address_Subscript = 0;
#if defined(PPP)
   Address_Subscript = i * getSerialDomain().Stride[0] + 
                       j * getSerialDomain().Stride[1] * getSerialDomain().Size[0];
#else
   Address_Subscript = i * Array_Descriptor.Array_Domain.Stride[0] + 
                       j * Array_Descriptor.Array_Domain.Stride[1] * 
		       Array_Descriptor.Array_Domain.Size[0];
#endif

#if defined(PPP)
   // A++ code
   // The determination of a value being off processor or not should INCLUDE ghost 
   // boundaries so that assignment to a value accessed via scalar indexing is 
   // updated on it's ghost boundaries too.  But then only the one processor should 
   // do the broadcast.
   // Bugfix (10/18/95) The bound of the SerialArray already includes the 
   // InternalGhostCellWidth for that axis!
   const int *InternalGhostCellWidth = 
      Array_Descriptor.Array_Domain.InternalGhostCellWidth;
   int *Local_Data_Base = getSerialDomain().Data_Base;
   int *Local_Base      = getSerialDomain().Base;
   //int *Local_Bound     = getSerialDomain().Bound;
   // ... need this too ...
   int *Local_Size          = getSerialDomain().Size;
   int *Local_Scalar_Offset = getSerialDomain().Scalar_Offset;
   int *Local_User_Base = getSerialDomain().User_Base;
   int *Local_Stride = getSerialDomain().Stride;

   // ... subscripts don't give real location since the stride for user is
   //  1 but we need the real location here ...
   int ii = (i-Local_User_Base[0]) * Local_Stride[0] + Local_Data_Base[0]+
      Local_Base[0];
   int jj = (j-Local_User_Base[1]) * Local_Stride[1] + Local_Data_Base[1]+
      Local_Base[1];

   /*
   // ... (bug fix, kdb, 6/4/96) Local_Data_Base already includes
   //  ghost cells, Local_Bound is only related to view so size must be
   //  used instead ...
   */

   bool Off_Processor_Including_Ghost_Boundaries = TRUE;
   bool Off_Processor_Excluding_Ghost_Boundaries = TRUE;

   if (!getSerialDomain().Is_A_Null_Array)
   {

      Off_Processor_Including_Ghost_Boundaries =
         (((ii<Local_Data_Base[0]) ||
           (ii>Local_Data_Base[0]+Local_Size[0]-1)) ||
          ((jj<Local_Data_Base[1]) ||
           (jj>Local_Data_Base[1]+(Local_Size[1]/Local_Size[0])-1)));

      Off_Processor_Excluding_Ghost_Boundaries =
         (((ii<Local_Data_Base[0]+InternalGhostCellWidth[0]) || 
           (ii>Local_Data_Base[0]+Local_Size[0]-1
              -InternalGhostCellWidth[0])) ||
          ((jj<Local_Data_Base[1]+InternalGhostCellWidth[1]) || 
           (jj>Local_Data_Base[1]+(Local_Size[1]/Local_Size[0])-1
              -InternalGhostCellWidth[1])));
   }

   if (Optimization_Manager::Optimize_Scalar_Indexing == FALSE)
      Scalar_Indexing_For_intArray_With_Message_Passing 
	 (Address_Subscript+Local_Scalar_Offset[1], 
	  Off_Processor_Including_Ghost_Boundaries,
          Off_Processor_Excluding_Ghost_Boundaries,
	  Array_Index_For_int_Variable);
      else
       {
         APP_ASSERT (Array_Index_For_int_Variable <= STATIC_LIMIT_FOR_PARALLEL_SCALAR_INDEXING_REFERENCES);
         if (Array_Index_For_int_Variable == STATIC_LIMIT_FOR_PARALLEL_SCALAR_INDEXING_REFERENCES)
              Array_Index_For_int_Variable = 0;
       }

   APP_ASSERT (Array_Index_For_int_Variable < STATIC_LIMIT_FOR_PARALLEL_SCALAR_INDEXING_REFERENCES);
   APP_ASSERT (getSerialArrayDescriptor().Array_View_Pointer1 == 
       getSerialArrayDescriptor().Array_Data + getSerialDomain().Scalar_Offset[1]);
   return (Off_Processor_Including_Ghost_Boundaries) ? 
           Static_int_Variable[Array_Index_For_int_Variable++] : 
           getSerialArrayDescriptor().Array_View_Pointer1 [Address_Subscript];
#else
// A++ code
   APP_ASSERT (Array_Descriptor.Array_View_Pointer1 == 
	       Array_Descriptor.Array_Data + Array_Descriptor.Array_Domain.Scalar_Offset[1]);
   return Array_Descriptor.Array_View_Pointer1 [Address_Subscript];
#endif
}

\end{lstlisting}

\newpage
\subsection{Where's the MPI}
    The scalar indexing is the only place in P++ where MPI calls are explicitly made.  All other
MPI calls occur within lower levesl of Block PARTI or PADRE.

\vspace{0.5in}

\begin{lstlisting}{}
void
intArray::Scalar_Indexing_For_intArray_With_Message_Passing (
          int Address_Subscript, bool Off_Processor_With_Ghost_Boundaries,
          bool Off_Processor_Without_Ghost_Boundaries, int & Array_Index_For_References) const
   {
  // Send of receive messages as required for scalar indexing in parallel
  // Basically we go and get the data if it is on another processor OR we
  // broadcast the data if we own the data.  It is simple -- but expensive!
  // In the case of Optimization_Manager::Optimize_Scalar_Indexing == TRUE we can
  // skip this message passing and just return a reference to the data element
  // if we own it or a dummy variable if we don't.

#if defined(PPP)
  // This code is only important for P++.  It handles the message passing required to support
  // the scalar indexing (if it is left unoptimized).

#if COMPILE_DEBUG_STATEMENTS
     if (APP_DEBUG > 0)
        {
          printf ("Off_Processor_With_Ghost_Boundaries            = %s \n",(Off_Processor_With_Ghost_Boundaries)    ? "TRUE" : "FALSE" );
          printf ("Off_Processor_Without_Ghost_Boundaries         = %s \n",(Off_Processor_Without_Ghost_Boundaries) ? "TRUE" : "FALSE" );
          printf ("Optimization_Manager::Optimize_Scalar_Indexing = %s \n",(Optimization_Manager::Optimize_Scalar_Indexing) ? "TRUE" : "FALSE" );
        }
#endif

#if COMPILE_DEBUG_STATEMENTS
     Communication_Manager::Sync();
#endif

     MPI_Status status;
     int return_status                                         = MPI_SUCCESS;
     int GLOBAL_MPI_MESSAGE_TYPE_FOR_SCALAR_INDEXING_BROADCAST = 503;
     int My_Rank;
     int  store_to_check;
     int  temp_store;

  // figure out the node number using MPI calls

     return_status = MPI_Comm_rank(MPI_COMM_WORLD, &My_Rank);
     APP_ASSERT(return_status == MPI_SUCCESS);

  // We have to be able to reference many values simultaniously so we keep them in
  // an array and cycle through the array as a circular list.

     APP_ASSERT (Array_Index_For_References <= STATIC_LIMIT_FOR_PARALLEL_SCALAR_INDEXING_REFERENCES);
     if (Array_Index_For_References >= STATIC_LIMIT_FOR_PARALLEL_SCALAR_INDEXING_REFERENCES)
        {
#if COMPILE_DEBUG_STATEMENTS
          if (APP_DEBUG > 0)
               printf ("Resetting Array_Index_For_References to ZERO! \n");
#endif
          Array_Index_For_References = 0;
        }

     if ( (Off_Processor_Without_Ghost_Boundaries == FALSE) && (My_Rank != 0) )
        {
       // PARTI does not INCLUDE any broadcasts as part of it's message passing
       // so we write this explicitly using MPI calls

       // now we are on the processor that owns the data and the rank of that processor
       // is not zero so we have to send that data to processor 0 in order to bcast it
       // to all the others

          temp_store = Array_Descriptor.SerialArray->Array_Descriptor.
	     Array_Data[Address_Subscript];

#if COMPILE_DEBUG_STATEMENTS
          if (APP_DEBUG > 0)
               printf ("ON PROCESSOR CASE (without ghost boundaries)! rank != 0 \n");
#endif
       // first we have to send the data to node 0 in order to make sure that
       // every node knows which node is the root-node during the broadcast
       // if we are already on node 0 we can proceed

#if defined(USE_PADRE)
          return_status = MPI_Send((void*)(&temp_store),
                                   1, MPI_INT, 0, 
                                   GLOBAL_MPI_MESSAGE_TYPE_FOR_SCALAR_INDEXING_BROADCAST,
                                   PADRE::MPICommunicator());
#else
          return_status = MPI_Send((void*)(&temp_store),
                                   1, MPI_INT, 0, 
                                   GLOBAL_MPI_MESSAGE_TYPE_FOR_SCALAR_INDEXING_BROADCAST,
                                   Global_PARTI_PADRE_Interface_PADRE_Comm_World);
#endif

          APP_ASSERT(return_status == MPI_SUCCESS);

       // store value that has been sent in order to check later whether the same value
       // got received

       // Static_Variable array hasn't been set yet so this is an error
       // store_to_check = Static_int_Variable[Array_Index_For_References];
          store_to_check = temp_store; 
        }

     if ( (Off_Processor_Without_Ghost_Boundaries == TRUE) && (My_Rank == 0) )
        {
       // on node 0 we have to receive that data 

          return_status = MPI_Recv((void*)(&temp_store),
                                   1, MPI_INT, MPI_ANY_SOURCE, 
                                   GLOBAL_MPI_MESSAGE_TYPE_FOR_SCALAR_INDEXING_BROADCAST,
#if defined(USE_PADRE)
                                   PADRE::MPICommunicator(),
#else
                                   Global_PARTI_PADRE_Interface_PADRE_Comm_World,
#endif
                                   &status);

          APP_ASSERT(return_status == MPI_SUCCESS);
        }

     if ( (Off_Processor_Without_Ghost_Boundaries == FALSE) && (My_Rank == 0) )
       {
      // this is the case where processor zero actually has the data so we only need to
      // copy it the the right place in order to broadcast it
	 temp_store = Array_Descriptor.SerialArray->Array_Descriptor.Array_Data[Address_Subscript];
       }
     
     // now node 0 has the data in temp_store and we can proceed with a broadcast
     // where root is node 0

        return_status = MPI_Bcast((void*)(&temp_store), 1, MPI_INT, 0, MPI_COMM_WORLD);

        APP_ASSERT(return_status == MPI_SUCCESS);

  // now all the nodes should contain the same value in temp_store but oly the ones that
  // did not own the data in the first place have to copy it
     if (Off_Processor_Without_Ghost_Boundaries == TRUE)  
	 Static_int_Variable[Array_Index_For_References] = temp_store;
#endif
     }

\end{lstlisting}



\end{document}

